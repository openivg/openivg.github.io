<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="InViG: Open-Ended Interactive Visual Grounding in Human-Robot Interaction with 500K Dialogues">
  <meta name="keywords" content="Interactive Visual Grounding, Open-Ended Human-Robot Interaction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>InViG: Open-Ended Interactive Visual Grounding in Human-Robot Interaction with 500K Dialogues</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">InViG: Open-Ended Interactive Visual Grounding in Human-Robot Interaction with 500K Dialogues</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Hanbo Zhang<sup>*</sup>,
            </span>
            <span class="author-block">
              Jie Xu<sup>*</sup>,
            </span>
            <span class="author-block">
              Yuchen Mo,
            </span>
            <span class="author-block">
              Tao Kong,
            </span>
          </div>

          <br>

          <div class="is-size-5 publication-authors">
            <span class="author-block">ByteDance Research &nbsp;&nbsp;</span>
            <span class="author-block"><sup>*</sup>Equal Contribution </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="static/paper/invig.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://github.com/openivg/openivg.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/openivg/openivg.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>GitHub</span>
              </a>
            </span>

            <span class="link-block">
              <a href="static/paper/invig_files.zip"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-download"></i>
                </span>
                <span>Dataset</span>
              </a>
            </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src=""
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!-- /Paper video. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Ambiguity is ubiquitous in human communication. Previous approaches in Human-Robot Interaction (HRI) have
            often relied on predefined interaction templates, leading to reduced performance in realistic and open-ended
            scenarios. To address these issues, we present a large-scale dataset, InViG, for interactive visual
            grounding under language ambiguity. Our dataset comprises over 520K images accompanied by open-ended
            goal-oriented disambiguation dialogues, encompassing millions of object instances and corresponding
            question-answer pairs. Leveraging the InViG dataset, we conduct extensive studies and propose a set of baseline
            solutions for end-to-end interactive visual disambiguation and grounding, achieving a 45.6\% success rate
            during validation. To the best of our knowledge, the InViG dataset is the first large-scale dataset for
            resolving open-ended interactive visual grounding, presenting a practical yet highly challenging benchmark
            for ambiguity-aware HRI.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method -->
     <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">InViG 520K Dataset</h2>
        <h3 class="has-text-centered">
          <div class="content has-text-justified">
          <p>
            We first sample and filter 21K images from OpemImages Dataset, based on which we recruit annotators to
            label each image with one or more targets and human-to-human dialogues. With 21K labeled data, we further
            develop an annotation system to automatically generate HRI data. We further generate 500K goal-oriented
            disambiguation dialogues in extremely low costs. Therefore, in total, our InViG dataset contains more than
            520K dialogues for interactive visual grounding. We demonstrate the comparison between InViG Dataset and
            previous works in Table I. In summary, InViG dataset is proposed to solve the problem of object-oriented
            open-ended interactive ambiguity in HRI, which widely appears in daily communications between humans.
            Therefore, differentiated from all previous works, InViG dataset contains extensive interactive disambi
            guation data to facilitate the development of HRI systems. We also show the statistics of InViG 520K in
            Figure 1.
          </p>
        </div>
        </h3>
        <br>
        <img id="comparison" width="100%" src="static/images/comptable.png">
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
<!--        <h2 class="title is-3">Samples</h2>-->
        <h3 class="has-text-centered">
          <div class="content has-text-justified">
          <p>
            Some samples from InViG 21K and InViG 500K:
          </p>
        </div>
        </h3>
        <br>
        <img id="invig samples" width="100%" src="static/images/examples.jpg">
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method -->
     <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Interactive Visual Grounding Demos</h2>
        <h3 class="has-text-centered">
          <div class="content has-text-justified">
          <p>
            We have demonstrated some demos of open-ended interactive demos here generated by our baseline
            solutions. The initial input of our models only includes an image and an initial referential
            expression. All other dialogue contents and bounding boxes are generated automatically by our models
            of the Guessor, Questioner, and Oracle. Bounding box candidates of objects are generated using
            <a href="https://github.com/facebookresearch/Detic">Detic</a> detector.
          </p>
        </div>
        </h3>
        <br>
        <img id="demos" width="100%" src="static/images/self_play_demos.jpg">
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
    @inproceedings{zhangxu2023invig
      author    = {Zhang, Hanbo and Xu, Jie and Mo, Yuchen and Kong, Tao},
      title     = {InViG: Open-Ended Interactive Visual Grounding in Human-Robot Interaction with 500K Dialogues},
      year      = {2023}
    }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
        <div class="content">
          <p>
            The website template was adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
  </div>
</footer>

</body>
</html>
